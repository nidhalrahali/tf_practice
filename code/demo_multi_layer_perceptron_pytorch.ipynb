{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"This notebook use [%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(in_features=3, h_dims=[256,256], actv=nn.ReLU, out_actv=nn.ReLU):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron \n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    in_features = in_features\n",
    "    for h_dim in h_dims[:-1]:\n",
    "        \n",
    "        ki = torch.randn(h_dim, in_features, dtype=torch.float, device=device, requires_grad=True)\n",
    "        \n",
    "        linear = nn.Linear(in_features, h_dim).to(device)\n",
    "        linear.weight = nn.Parameter(ki)\n",
    "        layers.append(linear)\n",
    "        \n",
    "        act = actv(inplace=True)\n",
    "        layers.append(act)\n",
    "\n",
    "        in_features = h_dim\n",
    "        \n",
    "    ki = torch.randn(h_dims[-1], in_features, dtype=torch.float, device=device, requires_grad=True)\n",
    "    linear = nn.Linear(in_features, h_dims[-1]).to(device)\n",
    "    linear.weight = nn.Parameter(ki)\n",
    "    layers.append(linear)\n",
    "    \n",
    "    if out_actv:\n",
    "        act = out_actv(inplace=True)\n",
    "        layers.append(act)\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('../data/mnist_data/',\n",
    "                             download=True,\n",
    "                             train=True) \n",
    "x_train = train_dataset.data.unsqueeze(1)/255.\n",
    "x_train = torch.flatten(x_train, 1)\n",
    "y_train = train_dataset.targets\n",
    "n_train = train_dataset.data.shape[0]\n",
    "\n",
    "test_dataset = datasets.MNIST(\"../data/mnist_data/\", \n",
    "                             download=True,\n",
    "                             train=False)\n",
    "\n",
    "x_test = test_dataset.data.unsqueeze(1)/255.\n",
    "x_test = torch.flatten(x_test, 1)\n",
    "y_test = test_dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClsClass(object):\n",
    "    \"\"\"\n",
    "    MLP for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, name='mlp', x_dim=784, y_dim=10, h_dims=[128,128], actv=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Initialize\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.h_dims = h_dims\n",
    "        self.actv = actv\n",
    "        self.build_model()\n",
    "        self.main_vars = sum([parameter.numel() for parameter in self.net.parameters()])\n",
    "        print(\"[%s] instantiated.\"%(self.name))\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build model\n",
    "        \"\"\"\n",
    "        self.net = mlp(in_features=self.x_dim, h_dims=self.h_dims+[self.y_dim],\n",
    "                         actv=self.actv, out_actv=None)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=0.001)\n",
    "    \n",
    "    def update(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Update model \n",
    "        \"\"\"\n",
    "        y_pred = self.net(x_batch)\n",
    "        cost_val = self.loss(y_pred, y_batch)\n",
    "        self.optim.zero_grad()\n",
    "        cost_val.backward()\n",
    "        self.optim.step()\n",
    "        return cost_val\n",
    "    \n",
    "    def test(self, test_x, test_y, batch_size):\n",
    "        \"\"\"\n",
    "        Test the model\n",
    "        \"\"\"\n",
    "        n_test = len(x_test)\n",
    "        p_idx = np.random.permutation(n_test)\n",
    "        max_iter = np.ceil(n_test/batch_size).astype(np.int) # number of iterations\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for it in range(max_iter):\n",
    "                b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "                x_batch, y_batch = test_x[b_idx].to(device), test_y[b_idx].to(device)\n",
    "                y_pred = self.net(x_batch)\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "            val_acc = (100 * correct / total)\n",
    "        return val_acc\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = MultiLayerPerceptronClsClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_idx,(name, var) in enumerate(M.net.named_parameters()):\n",
    "    print (v_idx, name, var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch,batch_size,print_every = 20,128,1\n",
    "\n",
    "max_iter = np.ceil(n_train/batch_size).astype(np.int) # number of iterations\n",
    "for epoch in range(max_epoch):\n",
    "    p_idx = np.random.permutation(n_train)\n",
    "    cost_val_sum,cnt = 0,0\n",
    "    for it in range(max_iter):\n",
    "        b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "        x_batch,y_batch = x_train[b_idx].to(device), y_train[b_idx].to(device)\n",
    "        cost_val = M.update(x_batch=x_batch,y_batch=y_batch)\n",
    "        cost_val_sum += cost_val*len(b_idx)\n",
    "        cnt += len(b_idx)\n",
    "    cost_val_avg = cost_val_sum / cnt\n",
    "    if ((epoch%print_every)==0) or (epoch==(max_epoch-1)):\n",
    "        accr_val = M.test(x_test, y_test, batch_size)\n",
    "        print (\"epoch:[%d/%d] cost:[%.3f] test_accuracy:[%.3f]\"%\n",
    "               (epoch,max_epoch,cost_val_avg,accr_val))\n",
    "print (\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
