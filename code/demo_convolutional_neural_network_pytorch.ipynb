{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PyTorch version:[1.3.1].\nThis notebook use [cpu].\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from util_pytorch import mlp\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"This notebook use [%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train:[60000], n_test:[10000], x_dim:[torch.Size([1, 28, 28])], y_dim:[torch.Size([60000])]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST('../data/mnist_data/',\n",
    "                             download=True,\n",
    "                             train=True) \n",
    "x_train = train_dataset.data.unsqueeze(1)/255.\n",
    "y_train = train_dataset.targets\n",
    "n_train = len(x_train)\n",
    "\n",
    "test_dataset = datasets.MNIST(\"../data/mnist_data/\", \n",
    "                             download=True,\n",
    "                             train=False)\n",
    "\n",
    "x_test = test_dataset.data.unsqueeze(1)/255.\n",
    "y_test = test_dataset.targets\n",
    "n_test = len(x_test)\n",
    "print (\"n_train:[%d], n_test:[%d], x_dim:[%s], y_dim:[%s]\"%\n",
    "       (n_train,n_test,x_train.shape[1:],y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv -> BN -> actv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((128, 1, 28, 28), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "layer_list.append(nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1))\n",
    "layer_list.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "layer_list.append(nn.BatchNorm2d(32))\n",
    "layer_list.append(nn.ReLU(inplace=True))\n",
    "\n",
    "module1 = nn.Sequential(*layer_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = module1(test_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "layer_list.append(nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1))\n",
    "layer_list.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "layer_list.append(nn.BatchNorm2d(32))\n",
    "layer_list.append(nn.ReLU(inplace=True))\n",
    "\n",
    "module2 = nn.Sequential(*layer_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = module2(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1568])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = flatten(out)\n",
    "print(out.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mlp(1568,h_dims=[128,10],actv=nn.ReLU,out_actv=None,\n",
    "          USE_DROPOUT=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = net(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(*[module1, module2, flatten, net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(net, x_batch, y_batch, cost, optim):\n",
    "    \"\"\"\n",
    "    Update model \n",
    "    \"\"\"\n",
    "    y_pred = net(x_batch)\n",
    "    cost_val = cost(y_pred, y_batch)\n",
    "    optim.zero_grad()\n",
    "    cost_val.backward()\n",
    "    optim.step()\n",
    "    return cost_val\n",
    "\n",
    "def get_accr(net, x, y, batch_size, device):\n",
    "        \"\"\"\n",
    "        Test the model\n",
    "        \"\"\"\n",
    "        n_test = len(x_test)\n",
    "        p_idx = np.random.permutation(n_test)\n",
    "        max_iter = np.ceil(n_test/batch_size).astype(np.int) # number of iterations\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for it in range(max_iter):\n",
    "                b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "                x_batch, y_batch = x[b_idx].to(device), y[b_idx].to(device)\n",
    "                y_pred = net(x_batch)\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "            acc = (100 * correct / total)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0/20] train_accuracy:[54.450] test_accuracy:[55.100]\n",
      "epoch:[1/20] train_accuracy:[46.380] test_accuracy:[46.210]\n",
      "epoch:[2/20] train_accuracy:[40.260] test_accuracy:[40.620]\n",
      "epoch:[3/20] train_accuracy:[38.060] test_accuracy:[38.820]\n",
      "epoch:[4/20] train_accuracy:[40.010] test_accuracy:[39.490]\n",
      "epoch:[5/20] train_accuracy:[42.570] test_accuracy:[41.770]\n",
      "epoch:[6/20] train_accuracy:[44.120] test_accuracy:[44.560]\n",
      "epoch:[7/20] train_accuracy:[47.260] test_accuracy:[46.730]\n",
      "epoch:[8/20] train_accuracy:[50.050] test_accuracy:[49.480]\n",
      "epoch:[9/20] train_accuracy:[52.530] test_accuracy:[52.430]\n",
      "epoch:[10/20] train_accuracy:[55.490] test_accuracy:[54.830]\n",
      "epoch:[11/20] train_accuracy:[59.810] test_accuracy:[57.920]\n",
      "epoch:[12/20] train_accuracy:[61.350] test_accuracy:[60.520]\n",
      "epoch:[13/20] train_accuracy:[64.240] test_accuracy:[64.370]\n",
      "epoch:[14/20] train_accuracy:[67.430] test_accuracy:[66.250]\n",
      "epoch:[15/20] train_accuracy:[69.110] test_accuracy:[68.160]\n",
      "epoch:[16/20] train_accuracy:[70.850] test_accuracy:[70.310]\n",
      "epoch:[17/20] train_accuracy:[72.390] test_accuracy:[71.480]\n",
      "epoch:[18/20] train_accuracy:[73.620] test_accuracy:[73.530]\n",
      "epoch:[19/20] train_accuracy:[75.830] test_accuracy:[74.210]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cost = nn.CrossEntropyLoss()\n",
    "optm = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "max_epoch,batch_size,print_every = 20,128,1\n",
    "max_iter = np.ceil(n_train/batch_size).astype(np.int) # number of iterations\n",
    "for epoch in range(max_epoch):\n",
    "    p_idx = np.random.permutation(n_train)\n",
    "    for it in range(max_iter):\n",
    "        b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "        x_batch,y_batch = x_train[b_idx].to(device),y_train[b_idx].to(device)\n",
    "        \n",
    "        update(cnn, x_batch, y_batch, cost, optm)\n",
    "        \n",
    "    if ((epoch%print_every)==0) or (epoch==(max_epoch-1)):\n",
    "        train_accr_val = get_accr(cnn, x_train, y_train, batch_size, device)\n",
    "        test_accr_val = get_accr(cnn, x_test, y_test, batch_size, device)\n",
    "        print (\"epoch:[%d/%d] train_accuracy:[%.3f] test_accuracy:[%.3f]\"%\n",
    "               (epoch,max_epoch,train_accr_val,test_accr_val))\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "class ConvNetClsClass(object):\n",
    "    \"\"\"\n",
    "    CNN for classification\n",
    "    \"\"\"\n",
    "    def __init__(self,name='CNN',img_dim=[1,28,28], y_dim=10,\n",
    "                 filter_sizes=[32,32],kernel_sizes=[3,3],h_dims=[128],\n",
    "                 USE_BN=True, USE_DROPOUT=True, device=None):\n",
    "        self.name = name\n",
    "        self.y_dim = y_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.downsample_ratio = len(filter_sizes)*2\n",
    "        \n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.h_dims = h_dims\n",
    "        \n",
    "        self.USE_BN = USE_BN\n",
    "        self.USE_DROPOUT = USE_DROPOUT\n",
    "        \n",
    "        self.device = device\n",
    "        self.build_model()\n",
    "        self.main_vars = sum([parameter.numel() for parameter in self.net.parameters()])\n",
    "        print(\"[%s] instantiated.\"%(self.name))\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build model\n",
    "        \"\"\"\n",
    "        # Conv layers\n",
    "        in_features = self.img_dim[0]\n",
    "        layer_list = []\n",
    "        for (filter_size,kernel_size) in zip(self.filter_sizes,self.kernel_sizes):\n",
    "            layer_list.append(nn.Conv2d(in_features, filter_size, kernel_size=3, stride=1, padding=1))\n",
    "            layer_list.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            if self.USE_BN:\n",
    "                layer_list.append(nn.BatchNorm2d(filter_size))\n",
    "            layer_list.append(nn.ReLU(inplace=True))\n",
    "            in_features = filter_size\n",
    "        conv_block = nn.Sequential(*layer_list)\n",
    "        \n",
    "        # Dense layers\n",
    "        layer_list = []\n",
    "        layer_list.append(nn.Flatten())\n",
    "        in_features = (self.img_dim[1]//self.downsample_ratio) * (self.img_dim[2]//self.downsample_ratio) * in_features\n",
    "        layer_list.append(mlp(in_features,h_dims=self.h_dims+[self.y_dim],actv=nn.ReLU,out_actv=None,\n",
    "                                      USE_DROPOUT=True, device=None))\n",
    "        dense = nn.Sequential(*layer_list)\n",
    "        \n",
    "        self.net = nn.Sequential(*[conv_block, dense]).to(self.device)\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=0.001)\n",
    "        \n",
    "        \n",
    "    def update(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Update model \n",
    "        \"\"\"\n",
    "        y_pred = self.net(x_batch)\n",
    "        cost_val = self.cost(y_pred, y_batch)\n",
    "        self.optim.zero_grad()\n",
    "        cost_val.backward()\n",
    "        self.optim.step()\n",
    "        return cost_val\n",
    "    \n",
    "    def get_accr(self, x, y, batch_size):\n",
    "        \"\"\"\n",
    "        Test the model\n",
    "        \"\"\"\n",
    "        n_test = len(x_test)\n",
    "        p_idx = np.random.permutation(n_test)\n",
    "        max_iter = np.ceil(n_test/batch_size).astype(np.int) # number of iterations\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for it in range(max_iter):\n",
    "                b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "                x_batch, y_batch = x[b_idx].to(self.device), y[b_idx].to(self.device)\n",
    "                y_pred = self.net(x_batch)\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "            acc = (100 * correct / total)\n",
    "        return acc\n",
    "        \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] instantiated.\n"
     ]
    }
   ],
   "source": [
    "C = ConvNetClsClass(name='CNN',y_dim=10,img_dim=[1,28,28],\n",
    "                    filter_sizes=[32,32],kernel_sizes=[3,3],h_dims=[128],\n",
    "                    USE_BN=True,USE_DROPOUT=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0.weight torch.Size([32, 1, 3, 3])\n",
      "1 0.0.bias torch.Size([32])\n",
      "2 0.2.weight torch.Size([32])\n",
      "3 0.2.bias torch.Size([32])\n",
      "4 0.4.weight torch.Size([32, 32, 3, 3])\n",
      "5 0.4.bias torch.Size([32])\n",
      "6 0.6.weight torch.Size([32])\n",
      "7 0.6.bias torch.Size([32])\n",
      "8 1.1.0.weight torch.Size([128, 1568])\n",
      "9 1.1.0.bias torch.Size([128])\n",
      "10 1.1.3.weight torch.Size([10, 128])\n",
      "11 1.1.3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for v_idx,(name, var) in enumerate(C.net.named_parameters()):\n",
    "    print (v_idx, name, var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1/20] cost:[21.085] test_accuracy:[34.990]\n",
      "epoch:[2/20] cost:[1.841] test_accuracy:[44.190]\n",
      "epoch:[3/20] cost:[1.571] test_accuracy:[51.740]\n",
      "epoch:[4/20] cost:[1.411] test_accuracy:[56.490]\n",
      "epoch:[5/20] cost:[1.287] test_accuracy:[60.250]\n",
      "epoch:[6/20] cost:[1.172] test_accuracy:[62.430]\n",
      "epoch:[7/20] cost:[1.097] test_accuracy:[65.110]\n",
      "epoch:[8/20] cost:[1.007] test_accuracy:[68.460]\n",
      "epoch:[9/20] cost:[0.927] test_accuracy:[70.270]\n",
      "epoch:[10/20] cost:[0.828] test_accuracy:[73.660]\n",
      "epoch:[11/20] cost:[0.776] test_accuracy:[76.280]\n",
      "epoch:[12/20] cost:[0.727] test_accuracy:[76.690]\n",
      "epoch:[13/20] cost:[0.672] test_accuracy:[79.300]\n",
      "epoch:[14/20] cost:[0.641] test_accuracy:[80.070]\n",
      "epoch:[15/20] cost:[0.593] test_accuracy:[82.130]\n",
      "epoch:[16/20] cost:[0.571] test_accuracy:[83.010]\n",
      "epoch:[17/20] cost:[0.541] test_accuracy:[82.730]\n",
      "epoch:[18/20] cost:[0.500] test_accuracy:[84.570]\n",
      "epoch:[19/20] cost:[0.478] test_accuracy:[85.650]\n",
      "epoch:[20/20] cost:[0.444] test_accuracy:[86.030]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_epoch,batch_size,print_every = 20,128,1\n",
    "for epoch in range(max_epoch):\n",
    "    p_idx = np.random.permutation(n_train)\n",
    "    cost_val_sum,cnt = 0,0\n",
    "    for it in range(max_iter):\n",
    "        b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "        x_batch,y_batch = x_train[b_idx].to(device), y_train[b_idx].to(device)\n",
    "        cost_val = C.update(x_batch=x_batch,y_batch=y_batch)\n",
    "        cost_val_sum += cost_val*len(b_idx)\n",
    "        cnt += len(b_idx)\n",
    "    cost_val_avg = cost_val_sum / cnt\n",
    "    if ((epoch%print_every)==0) or (epoch==(max_epoch-1)):\n",
    "        accr_val = C.get_accr(x_test, y_test, batch_size)\n",
    "        print (\"epoch:[%d/%d] cost:[%.3f] test_accuracy:[%.3f]\"%\n",
    "               (epoch+1,max_epoch,cost_val_avg,accr_val))\n",
    "print (\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tc': pyenv)",
   "language": "python",
   "name": "python_defaultSpec_1601569955007"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}